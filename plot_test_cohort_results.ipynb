{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image as pil_image\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.applications.mobilenet import MobileNet, relu6, DepthwiseConv2D\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import AveragePooling2D, Conv2D, UpSampling2D\n",
    "from keras.models import load_model, Model\n",
    "from utils.keras_utils import w_accuracy, preprocess_input_tf, center_crop\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, figname=None, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    (This function is copied from the scikit docs.)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7,7))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=18)\n",
    "    plt.yticks(tick_marks, classes, fontsize=18)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, '%.2f' % cm[i, j], horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                     fontsize=16)\n",
    "        else:\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                     fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    #plt.ylabel('True label', fontsize=20)\n",
    "    #plt.xlabel('Predicted label', fontsize=20)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    # plt.colorbar()\n",
    "    if figname is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(figname)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stats(y_true, y_pred, n_class=4):\n",
    "    col = sns.color_palette()\n",
    "\n",
    "    fig, axarr = plt.subplots(n_class, n_class, sharex=True, figsize=(10,10))\n",
    "    for i_true in range(n_class):\n",
    "        v = y_pred[y_true == i_true]\n",
    "        for j_pred in range(n_class):\n",
    "            axarr[i_true, j_pred].hist(v[:, j_pred], 10, normed=False, color=col[i_true])\n",
    "            axarr[i_true, j_pred].set_title('class=%d, prediction=%d' % (i_true, j_pred))\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames_and_classes(csv_path):\n",
    "    df = pd.read_csv(csv_path, sep='\\t', index_col=0)\n",
    "    filenames, classes = [], []\n",
    "    for base_name, primary_grade, sec_grade in zip(df.index, df.iloc[:,0], df.iloc[:,1]):\n",
    "        primary_grade, sec_grade = int(primary_grade), int(sec_grade)\n",
    "        filenames.append(base_name)\n",
    "        classes.append(np.array([primary_grade, sec_grade]).reshape(1,2))\n",
    "    classes = np.vstack(classes)\n",
    "    return filenames, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_dim = 250\n",
    "target_dim = 224\n",
    "target_size = (target_dim, target_dim)\n",
    "input_shape = (target_size[0], target_size[1], 3)\n",
    "bs = 32\n",
    "\n",
    "# classes\n",
    "class_labels = ['Benign', 'Gleason 3', 'Gleason 4', 'Gleason 5']\n",
    "n_class = len(class_labels)\n",
    "prefix = '/data3/eirini/dataset_TMA'\n",
    "patch_dir = os.path.join(prefix, 'train_validation_patches_750')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "tma = 'ZT76'\n",
    "csv_path = os.path.join(prefix, 'tma_info', '%s_gleason_scores.csv' % tma)\n",
    "val_filenames, val_classes = get_filenames_and_classes(csv_path)\n",
    "print('Total TMAs in validation set: %d' % len(val_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_weights = 'model_weights/MobileNet_gleason_weights.h5'\n",
    "model = load_model(model_weights,\n",
    "                   custom_objects={'relu6': relu6,\n",
    "                                   'DepthwiseConv2D': DepthwiseConv2D,\n",
    "                                   'w_accuracy':w_accuracy\n",
    "                                       })\n",
    "\n",
    "outdir = '/data3/eirini/TMA_results/plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix for validation set\n",
    "val_patch_names = []\n",
    "val_patch_labels = []\n",
    "for val_subdir in val_filenames:\n",
    "    subpath = os.path.join(patch_dir, val_subdir)\n",
    "    for fname in os.listdir(subpath):\n",
    "        if fname.lower().endswith('.jpg'):\n",
    "            val_patch_names.append(os.path.join(subpath, fname))\n",
    "            y = int(fname.split('_')[-1].rstrip('.jpg'))\n",
    "            val_patch_labels.append(y)\n",
    "n_patch = len(val_patch_names)\n",
    "\n",
    "y_pred_prob = np.zeros((n_patch, n_class))\n",
    "for i, patch_name in enumerate(val_patch_names):\n",
    "    img = image.load_img(patch_name, grayscale=False, target_size=(init_dim, init_dim))\n",
    "    X = image.img_to_array(img)\n",
    "    X = center_crop(X, center_crop_size=target_size)\n",
    "    X = preprocess_input_tf(X)\n",
    "    y_pred_prob[i] = model.predict(X[np.newaxis,:,:,:], batch_size=1)[0]\n",
    "\n",
    "y_true = np.array(val_patch_labels)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kappa_val = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "print('%.2f' % kappa_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_stats(y_true, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# figname = os.path.join(outdir, 'validation_patch_based.eps')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, class_labels, figname=None, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test filenames\n",
    "test_patch_dir = os.path.join(prefix, 'inter_observer', 'joint_test_patches_750', 'patho_1')\n",
    "test_filenames = [f for f in os.listdir(test_patch_dir) if f.startswith('ZT80')]\n",
    "len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patches_csv_path = os.path.join(prefix, 'inter_observer', 'ZT80_patch_grades.csv')\n",
    "df_patch = pd.read_csv(patches_csv_path, sep='\\t', index_col=0)\n",
    "true_grades = df_patch.values\n",
    "\n",
    "# make predictions on test cohort patches\n",
    "y_pred_prob = np.zeros((df_patch.shape[0], n_class))\n",
    "for i, (patch_name, y_true) in enumerate(zip(df_patch.index, df_patch.iloc[:,0])):\n",
    "    spot_name = patch_name.split('_patch_')[0]\n",
    "    full_name = os.path.join(test_patch_dir, spot_name, patch_name+'_class_%d.jpg' % y_true)\n",
    "    img = image.load_img(full_name, grayscale=False, target_size=(init_dim, init_dim))\n",
    "    X = image.img_to_array(img)\n",
    "    X = center_crop(X, center_crop_size=target_size)\n",
    "    X = preprocess_input_tf(X)\n",
    "    y_pred_prob[i] = model.predict(X[np.newaxis,:,:,:], batch_size=1)[0]\n",
    "\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix for test set - CNN vs Kim's annotations\n",
    "\n",
    "# figname = os.path.join(outdir, 'CNN_vs_patho1_patch_based.eps')\n",
    "cm = confusion_matrix(true_grades[:,0], y_pred)\n",
    "plot_confusion_matrix(cm, class_labels, figname=None, normalize=True)\n",
    "\n",
    "kappa_p1 = cohen_kappa_score(true_grades[:,0], y_pred, weights='quadratic')\n",
    "print('%.2f' % kappa_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix for test set - CNN vs Jan's annotations\n",
    "\n",
    "# figname = os.path.join(outdir, 'CNN_vs_patho2_patch_based.eps')\n",
    "cm = confusion_matrix(true_grades[:,1], y_pred)\n",
    "plot_confusion_matrix(cm, class_labels, figname=None, normalize=True)\n",
    "\n",
    "kappa_p2 = cohen_kappa_score(true_grades[:,1], y_pred, weights='quadratic')\n",
    "print('%.2f' % kappa_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inter-pathologist variability\n",
    "cm = confusion_matrix(true_grades[:,0], true_grades[:,1])\n",
    "# figname = os.path.join(outdir, 'patho1_patho2.eps')\n",
    "plot_confusion_matrix(cm, class_labels, figname=None, normalize=True)\n",
    "\n",
    "#cm = confusion_matrix(true_grades[:,1], true_grades[:,0])\n",
    "#figname = os.path.join(outdir, 'patho2_patho1.eps')\n",
    "#plot_confusion_matrix(cm, class_labels, figname=None, normalize=True)\n",
    "\n",
    "kappa_p1_p2 = cohen_kappa_score(true_grades[:,0], true_grades[:,1], weights='quadratic')\n",
    "print('%.2f' % kappa_p1_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Venn diagrams\n",
    "from matplotlib_venn import venn3\n",
    "sns.set_palette(sns.color_palette(\"Set2\", 8))\n",
    "for i in range(n_class):\n",
    "    set_Kim = set(df_patch.index[true_grades[:,0] == i])\n",
    "    set_Jan = set(df_patch.index[true_grades[:,1] == i])\n",
    "    set_CNN = set(df_patch.index[y_pred == i])\n",
    "    \n",
    "    plt.figure()\n",
    "    v = venn3([set_Kim, set_Jan, set_CNN], ['pathologist 1', 'pathologist 2', 'model'])\n",
    "    for text in v.set_labels:\n",
    "        text.set_fontsize(18)\n",
    "    for text in v.subset_labels:\n",
    "        if text is not None:\n",
    "            text.set_fontsize(16)\n",
    "    plt.show()\n",
    "    #plt.savefig(os.path.join(outdir, 'venn3_class_%d.pdf' % i))\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_group(a, b):\n",
    "    # if both cancer and benign tissue are predicted, ignore benign tissue\n",
    "    if (a > 0) and (b == 0):\n",
    "        b = a\n",
    "    if (b > 0) and (a == 0):\n",
    "        a = b\n",
    "\n",
    "    # get the actual Gleason grade\n",
    "    a += 2\n",
    "    b += 2\n",
    "    if a+b <= 6:\n",
    "        return 1\n",
    "    elif a+b == 7:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def gleason_summary_wsum(y_pred, n_class=4, thres=None):\n",
    "    gleason_scores = y_pred.copy()\n",
    "    gleason_scores /= np.sum(gleason_scores)\n",
    "    # remove outlier predictions\n",
    "    if thres is not None:\n",
    "        gleason_scores[gleason_scores < thres] = 0\n",
    "    # and assign overall grade\n",
    "    idx = np.argsort(gleason_scores)[::-1]\n",
    "    primary_class = idx[0]\n",
    "    secondary_class = idx[1] if gleason_scores[idx[1]] > 0 else idx[0]\n",
    "    return assign_group(primary_class, secondary_class)\n",
    "\n",
    "def pil_resize(img, target_size):\n",
    "    hw_tuple = (target_size[1], target_size[0])\n",
    "    if img.size != hw_tuple:\n",
    "        img = img.resize(hw_tuple)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute model predictions (pixel-level probability maps) \n",
    "\n",
    "w_out, b_out = model.layers[-1].get_weights()\n",
    "w_out = w_out[np.newaxis,np.newaxis,:,:]\n",
    "\n",
    "# rescaling factor is 3\n",
    "big_dim = 1024\n",
    "base_model = MobileNet(include_top=False, weights=None,\n",
    "                       input_shape=(big_dim, big_dim, 3),\n",
    "                       alpha=.5, depth_multiplier=1, dropout=.2)\n",
    "block_name = 'conv_pw_13_relu'\n",
    "x_input = base_model.get_layer(block_name).output\n",
    "\n",
    "# average pooling instead of global pooling\n",
    "x = AveragePooling2D((7, 7), strides=(1,1), padding='same', name='avg_pool_top')(x_input)\n",
    "x = Conv2D(n_class, (1, 1), activation='softmax', padding='same')(x)\n",
    "x_out = UpSampling2D(size=(32, 32), name='upsample')(x)\n",
    "big_model = Model(base_model.input, x_out)\n",
    "big_model.load_weights(model_weights, by_name=True)\n",
    "big_model.layers[-2].set_weights([w_out, b_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on entire TMA spots for the validation cohort.\n",
    "\n",
    "image_dir = os.path.join(prefix, 'TMA_images')\n",
    "mask_dir = os.path.join(prefix, 'tissue_masks')\n",
    "D_val = dict()\n",
    "\n",
    "for fname in val_filenames:\n",
    "    full_imfile = os.path.join(image_dir, fname+'.jpg')\n",
    "    # get network predictions as heatmap\n",
    "    img = image.load_img(full_imfile, grayscale=False, target_size=(big_dim, big_dim))\n",
    "    X = image.img_to_array(img)\n",
    "    X = preprocess_input_tf(X)\n",
    "    y_pred_prob = big_model.predict(X[np.newaxis,:,:,:], batch_size=1)[0]\n",
    "    \n",
    "    # get the (automatically generated) tissue mask\n",
    "    tissue_maskfile = os.path.join(mask_dir, 'mask_'+fname+'.png')\n",
    "    tissue_mask = pil_image.open(tissue_maskfile)\n",
    "    tissue_mask = np.array(pil_resize(tissue_mask, target_size=(big_dim, big_dim)))\n",
    "\n",
    "    # compute probability only at (predicted) tissue regions\n",
    "    y_pred_prob[tissue_mask == n_class] = 0.\n",
    "    y_pred_prob = y_pred_prob.reshape(-1, 4)\n",
    "    w_sum = np.sum(y_pred_prob, axis=0)\n",
    "    D_val[fname] = w_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrices and Cohen's kappa statistic for Gleason score assignments on entire TMA spots\n",
    "# (validation cohort)\n",
    "\n",
    "N = len(D_val.keys())\n",
    "csv_file = os.path.join(prefix, 'tma_info', 'ZT76_gleason_scores.csv')\n",
    "df_patho = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "# load training data survival times\n",
    "ii = 0\n",
    "x_gleason_cnn = np.zeros(N)\n",
    "x_gleason_annot_kim = np.zeros(N)\n",
    "case_ids = []\n",
    "\n",
    "for fname in D.keys():\n",
    "    # if the image was annotated by the pathologists\n",
    "    if fname in df_patho.index:\n",
    "        a, b = df_patho.loc[fname][['class_primary', 'class_secondary']]\n",
    "        x_gleason_annot_kim[ii] = assign_group(a, b)\n",
    "        x_gleason_cnn[ii] = gleason_summary_wsum(D[fname], thres=0.25)\n",
    "        case_ids.append(fname)\n",
    "        ii += 1\n",
    "    \n",
    "# discard unused part of matrices\n",
    "set_cnn = x_gleason_cnn[:ii]\n",
    "set_kim = x_gleason_annot_kim[:ii]\n",
    "print('Found survival annotation for %d out of %d TMA spots.' % (ii, N))\n",
    "\n",
    "# compute kappa values\n",
    "kappa_p1 = cohen_kappa_score(set_kim, set_cnn, weights='quadratic')\n",
    "print('CNN-pathologist 1: %.2f' % kappa_p1)\n",
    "\n",
    "classes = ['benign', 'Gleason 6', 'Gleason 7', 'Gleason 8', 'Gleason 9', 'Gleason 10']\n",
    "# figname = os.path.join(outdir, 'validation_TMA_based.eps')\n",
    "cm = confusion_matrix(set_kim, set_cnn)\n",
    "plot_confusion_matrix(cm, classes, figname=None, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on entire TMA spots for the test cohort.\n",
    "\n",
    "image_dir = os.path.join(prefix, 'TMA_images')\n",
    "mask_dir = os.path.join(prefix, 'tissue_masks')\n",
    "D = dict()\n",
    "\n",
    "for fname in test_filenames:\n",
    "    full_imfile = os.path.join(image_dir, fname+'.jpg')\n",
    "    # get network predictions as heatmap\n",
    "    img = image.load_img(full_imfile, grayscale=False, target_size=(big_dim, big_dim))\n",
    "    X = image.img_to_array(img)\n",
    "    X = preprocess_input_tf(X)\n",
    "    y_pred_prob = big_model.predict(X[np.newaxis,:,:,:], batch_size=1)[0]\n",
    "    \n",
    "    # get the (automatically generated) tissue mask\n",
    "    tissue_maskfile = os.path.join(mask_dir, 'mask_'+fname+'.png')\n",
    "    tissue_mask = pil_image.open(tissue_maskfile)\n",
    "    tissue_mask = np.array(pil_resize(tissue_mask, target_size=(big_dim, big_dim)))\n",
    "\n",
    "    # compute probability only at (predicted) tissue regions\n",
    "    y_pred_prob[tissue_mask == n_class] = 0.\n",
    "    y_pred_prob = y_pred_prob.reshape(-1, 4)\n",
    "    w_sum = np.sum(y_pred_prob, axis=0)\n",
    "    D[fname] = w_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrices and Cohen's kappa statistic for Gleason score assignments on entire TMA spots\n",
    "# (test cohort)\n",
    "\n",
    "N = len(D.keys())\n",
    "csv_file = os.path.join(prefix, 'tma_info', 'ZT80_gleason_scores.csv')\n",
    "df_patho = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "ii = 0\n",
    "x_gleason_cnn = np.zeros(N)\n",
    "x_gleason_annot_kim = np.zeros(N)\n",
    "x_gleason_annot_jan = np.zeros(N)\n",
    "case_ids = []\n",
    "\n",
    "for fname in D.keys():\n",
    "    # if the image was annotated by the pathologists\n",
    "    if fname in df_patho.index:\n",
    "        a, b = df_patho.loc[fname][['kim_class_primary', 'kim_class_secondary']]\n",
    "        x_gleason_annot_kim[ii] = assign_group(a, b)\n",
    "        a, b = df_patho.loc[fname][['jan_class_primary', 'jan_class_secondary']]\n",
    "        x_gleason_annot_jan[ii] = assign_group(a, b)\n",
    "        x_gleason_cnn[ii] = gleason_summary_wsum(D[fname], thres=0.25)\n",
    "        case_ids.append(fname)\n",
    "        ii += 1\n",
    "    \n",
    "# discard unused part of matrices\n",
    "set_cnn = x_gleason_cnn[:ii]\n",
    "set_kim = x_gleason_annot_kim[:ii]\n",
    "set_jan = x_gleason_annot_jan[:ii]\n",
    "print('Found survival annotation for %d out of %d TMA spots.' % (ii, N))\n",
    "\n",
    "# compute kappa values\n",
    "kappa_p1_p2 = cohen_kappa_score(set_kim, set_jan, weights='quadratic')\n",
    "print('inter-pathologist: %.2f' % kappa_p1_p2)\n",
    "kappa_p1 = cohen_kappa_score(set_kim, set_cnn, weights='quadratic')\n",
    "print('CNN-pathologist 1: %.2f' % kappa_p1)\n",
    "kappa_p2 = cohen_kappa_score(set_jan, set_cnn, weights='quadratic')\n",
    "print('CNN-pathologist 2: %.2f' % kappa_p2)\n",
    "\n",
    "# confusion matrices\n",
    "classes = ['benign', 'Gleason 6', 'Gleason 7', 'Gleason 8', 'Gleason 9', 'Gleason 10']\n",
    "\n",
    "#figname = os.path.join(outdir, 'CNN_vs_patho1_TMA_based.eps')\n",
    "cm = confusion_matrix(set_kim, set_cnn)\n",
    "plot_confusion_matrix(cm, classes, figname=None, normalize=False)\n",
    "\n",
    "#figname = os.path.join(outdir, 'CNN_vs_patho2_TMA_based.eps')\n",
    "cm = confusion_matrix(set_jan, set_cnn)\n",
    "plot_confusion_matrix(cm, classes, figname=None, normalize=False)\n",
    "\n",
    "#figname = os.path.join(outdir, 'patho1_vs_patho2_TMA_based.eps')\n",
    "cm = confusion_matrix(set_kim, set_jan)\n",
    "plot_confusion_matrix(cm, classes, figname=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in survival data\n",
    "\n",
    "df_surv = pd.read_csv(os.path.join(prefix, 'tma_info', 'filtered_survival_and_clinical_data.csv'),\n",
    "                      index_col=0)\n",
    "df_surv = df_surv.replace(to_replace=' ', value=np.nan)\n",
    "surv_pairs = [('os', 'st_os_gen'), ('os', 'st_os_spec'), ('rfs', 'rfs_status')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Organize Gleason score group assignments and survival data in compact dataframes\n",
    "# for downstream analysis in R\n",
    "\n",
    "N = len(D.keys())\n",
    "csv_file = os.path.join(prefix, 'tma_info', 'ZT80_gleason_scores.csv')\n",
    "df_patho = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "for surv_time, surv_status in surv_pairs:\n",
    "    print(surv_status)\n",
    "\n",
    "    # load training data survival times\n",
    "    ii = 0\n",
    "    y_surv = np.zeros((N, 2))\n",
    "    x_gleason_cnn = np.zeros((N, 1))\n",
    "    x_gleason_annot_kim = np.zeros((N, 1))\n",
    "    x_gleason_annot_jan = np.zeros((N, 1))\n",
    "    case_ids = []\n",
    "\n",
    "    for fname in D.keys():\n",
    "        # if survival information exists and image was annotated by both pathologists\n",
    "        if (fname in df_surv.index) and (fname in df_patho.index):\n",
    "            y_surv[ii] = df_surv.loc[fname][[surv_time, surv_status]]\n",
    "            a, b = df_patho.loc[fname][['kim_class_primary', 'kim_class_secondary']]\n",
    "            x_gleason_annot_kim[ii, 0] = assign_group(a, b)\n",
    "            a, b = df_patho.loc[fname][['jan_class_primary', 'jan_class_secondary']]\n",
    "            x_gleason_annot_jan[ii, 0] = assign_group(a, b)\n",
    "            x_gleason_cnn[ii, 0] = gleason_summary_wsum(D[fname], thres=0.25)\n",
    "            case_ids.append(fname)\n",
    "            ii += 1\n",
    "            \n",
    "    # discard unused part of matrices\n",
    "    y_surv = y_surv[:ii]\n",
    "    x_gleason_cnn = x_gleason_cnn[:ii]\n",
    "    x_gleason_annot_kim = x_gleason_annot_kim[:ii]\n",
    "    x_gleason_annot_jan = x_gleason_annot_jan[:ii]\n",
    "    print('Found survival information for %d out of %d TMA spots.' % (ii, N))\n",
    "    print('Events occurred: %d' % np.nansum(y_surv[:,1] == 1))\n",
    "    \n",
    "    # write out the dataframe\n",
    "    df = pd.DataFrame(np.hstack([y_surv, x_gleason_cnn, x_gleason_annot_kim, x_gleason_annot_jan]),\n",
    "                      columns=['time', 'status'] + ['GL_predicted', 'GL_annot_kim', 'GL_annot_jan'],\n",
    "                      index=case_ids)\n",
    "    df = df.dropna()\n",
    "    df.to_csv(os.path.join(outdir, '%s.csv' % surv_status), index=True, index_label='case_id')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Venn diagrams for Gleason score group assignments\n",
    "\n",
    "from matplotlib_venn import venn3\n",
    "risk_groups = ['low', 'intermediate', 'high']\n",
    "for _surv_time, surv_status in surv_pairs:\n",
    "    df = pd.read_csv(os.path.join(outdir, '%s.csv' % surv_status))\n",
    "    for i in range(1, 4):\n",
    "        print('%s-risk group' % risk_groups[i-1])\n",
    "        set_Kim = set(df.index[df['GL_annot_kim'] == i])\n",
    "        set_Jan = set(df.index[df['GL_annot_jan'] == i])\n",
    "        set_CNN = set(df.index[df['GL_predicted'] == i])\n",
    "\n",
    "        plt.figure()\n",
    "        v = venn3([set_Kim, set_Jan, set_CNN], ['pathologist 1', 'pathologist 2', 'model'])\n",
    "        for text in v.set_labels:\n",
    "            text.set_fontsize(18)\n",
    "        for text in v.subset_labels:\n",
    "            if text is not None:\n",
    "                text.set_fontsize(16)\n",
    "        plt.show()\n",
    "        #plt.savefig(os.path.join(outdir, 'venn3_%s_Gleason_group_%d.pdf' % (surv_status, i)))\n",
    "        #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute kappa values for Gleason score group assignment agreements\n",
    "for _surv_time, surv_status in surv_pairs:\n",
    "    df = pd.read_csv(os.path.join(outdir, '%s.csv' % surv_status))\n",
    "    set_Kim = np.array(df['GL_annot_kim'])\n",
    "    set_Jan = np.array(df['GL_annot_jan'])\n",
    "    set_CNN = np.array(df['GL_predicted'])\n",
    "    \n",
    "    kappa_p1_p2 = cohen_kappa_score(set_Kim, set_Jan, weights='quadratic')\n",
    "    print('inter-pathologist: %.2f' % kappa_p1_p2)\n",
    "    kappa_p1 = cohen_kappa_score(set_Kim, set_CNN, weights='quadratic')\n",
    "    print('CNN-pathologist 1: %.2f' % kappa_p1)\n",
    "    kappa_p2 = cohen_kappa_score(set_Jan, set_CNN, weights='quadratic')\n",
    "    print('CNN-pathologist 2: %.2f' % kappa_p2)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
